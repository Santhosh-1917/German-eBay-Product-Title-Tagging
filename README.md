


# ğŸ§  German eBay Product Title Tagging (NER using BiLSTM-CRF)

This project performs **Named Entity Recognition (NER)** on German eBay product titles.  
Each token in the product title is tagged with an appropriate aspect (e.g., *Hersteller*, *Farbe*, *Produktart*).  
The model learns to automatically label unseen product titles for structured extraction.

---

## ğŸ“‚ Project Overview

This repository implements a **BiLSTM-CRF** model for token-level sequence tagging across two product categories.  
It also supports optional integration of **FastText German word embeddings** for improved semantic understanding.

### ğŸ”§ Key Steps
1. **Data Preprocessing**
   - Loads and cleans the training dataset (`Tagged_Titles_Train.tsv`)
   - Replaces missing tags with most frequent tag per token (or `"O"`)
   - Splits by category and groups by record number

2. **Vocabulary & Encoding**
   - Builds token and tag mappings (`word2idx`, `tag2idx`)
   - Pads sentences and creates attention masks

3. **Model Architecture**
   - Embedding Layer (random or FastText)
   - Bidirectional LSTM
   - Linear projection â†’ tag space
   - Conditional Random Field (CRF) for structured decoding

4. **Training & Evaluation**
   - Early stopping, learning-rate scheduling, and F1 tracking
   - Macro F1-score, accuracy, and classification report per category

---

## ğŸ§© Model Architecture

Input â†’ Embedding (100â€“300d) â†’ BiLSTM â†’ Linear Layer â†’ CRF â†’ Tag Sequence

- **Embedding**: Random or pretrained (FastText German)  
- **Hidden Dim**: 128â€“256  
- **Optimizer**: Adam (lr = 1e-3)  
- **Regularization**: Dropout 0.3  
- **Loss Function**: Negative log-likelihood from CRF  
- **Metrics**: Macro F1-score, Accuracy  

---

## ğŸ“Š Results

| Category | Model Type | Validation F1 | Accuracy |
|-----------|-------------|---------------|-----------|
| Category 1 | BiLSTM-CRF | **0.78** | 0.93 |
| Category 2 | BiLSTM-CRF | **0.55** | 0.90 |

*(Expected improvement to ~0.82 F1 with FastText embeddings)*

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/<your-username>/German-eBay-Product-Title-Tagging.git
cd German-eBay-Product-Title-Tagging
pip install -r requirements.txt

Dependencies

torch
torchcrf
pandas
numpy
matplotlib
scikit-learn
gensim


â¸»

##ğŸš€ Training

Option 1 â€” Baseline (Random embeddings)

model_cat1, best_f1_cat1 = train_one_category(...)
model_cat2, best_f1_cat2 = train_one_category(...)

Option 2 â€” With FastText German embeddings

Download FastText German vectors:

cc.de.300.vec.gz

Then run:

cat1_emb_matrix = build_embedding_matrix(cat1_word2idx, ft_model)
model_cat1, best_f1_cat1 = train_one_category(..., pretrained_matrix=cat1_emb_matrix)


â¸»

ğŸ“ Dataset Description

Column	Description
Record Number	Unique ID per product title
Category	Product group (1 or 2)
Token	Word in the title
Tag	Labeled aspect (e.g. Hersteller, Farbe, GrÃ¶ÃŸe)


â¸»

ğŸ’¾ Outputs
	â€¢	bilstm_crf_cat1.pt â€” Trained model weights for Category 1
	â€¢	bilstm_crf_cat2.pt â€” Trained model weights for Category 2
	â€¢	submission.tsv â€” Predicted tags for unseen test titles

â¸»

ğŸ”® Future Work
	â€¢	Integrate FastText embeddings for semantic boost
	â€¢	Experiment with DistilBERT-German for contextual embeddings
	â€¢	Fine-tune learning rate and dropout
	â€¢	Add cross-category ensemble for higher robustness

â¸»

ğŸ§° Tech Stack


â¸»

ğŸ‘¨â€ğŸ’» Author

Santhosh Narayanan Baburaman
ğŸ“ M.S. Analytics @ University of Southern California
ğŸ“§ [your-email@example.com]
ğŸ”— LinkedIn | GitHub

â¸»


---

